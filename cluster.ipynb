{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "who",
   "display_name": "who"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Top terms per cluster:\nCluster 0:\ncanada, health, cdnpoli, pandemic, people, public, us, new, test, masks, today, help, please, need, approved, \nCluster 1:\nplease, editing, oph, tweets, btw, ant, impo, rather, posting, helpful, calling, tell, due, someone, updated, \nCluster 2:\nsources, information, info, consult, like, trustworthy, circulating, questionable, miracle, vaccinations, faster, internet, beware, cures, offers, \nCluster 3:\nepidemic, still, deck, releasing, strong, controls, models, technical, briefing, population, link, model, looking, deaths, thanks, \nCluster 4:\nbroadcast, facebook, channel, phac, edt, officials, live, pm, healthy, situation, update, canadians, today, senior, canada, \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# parameters for function\n",
    "name = 'GovCanHealth'\n",
    "lang = 'english'\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = re.sub('((www\\.[\\s]+)|(https?://[^\\s]+))', ' ', text)\n",
    "    text = re.sub('@[A-Za-z0-9_-]+', ' ', text)\n",
    "    text = re.sub('RT', ' ', text)\n",
    "    text = re.sub('#', ' ', text)\n",
    "    text = re.sub('[^\\w\\s]', ' ', text)\n",
    "    text = re.sub('[0-9]', ' ', text) # replace numbers\n",
    "    text = re.sub('\\s\\s+', ' ', text) # replace several spaces\n",
    "    return text\n",
    "\n",
    "original_tweets = []\n",
    "documents = []\n",
    "\n",
    "f = pd.read_json('data/qcri/' + name +'_geo.json', lines=True)\n",
    "for line in f['text']:\n",
    "    original_tweets.append(line)\n",
    "    documents.append(preprocessing(line))\n",
    "\n",
    "try:    \n",
    "    f = pd.read_csv('data/out/' + name + '.csv', 'rb', delimiter = '\\t')\n",
    "    for line in f['text']:\n",
    "        original_tweets.append(line)\n",
    "        documents.append(preprocessing(line))\n",
    "except:\n",
    "    print('No file in WHO data')\n",
    "\n",
    "\n",
    "# source â€” https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "stopwordsList = set(stopwords.words(lang)) # load list of stopwords of target language\n",
    "stopwordsList.add('covid') # here we add some covid-related words\n",
    "stopwordsList.add('covid19')\n",
    "stopwordsList.add('corona')\n",
    "stopwordsList.add('coronavirus')\n",
    "stopwordsList.add('covid-19')\n",
    "stopwordsList.add('amp')\n",
    "stopwordsList.add('via')\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwordsList)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "true_k = 5\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i),\n",
    "    text = ''\n",
    "    for ind in order_centroids[i, :15]:\n",
    "        text = text + str(terms[ind]) + ', '\n",
    "        #print(' %s' % terms[ind])\n",
    "    print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'[2]': 1293, '[0]': 17813, '[3]': 222, '[1]': 477, '[4]': 758}"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "source": [
    "freq = {}\n",
    "\n",
    "for d in documents:\n",
    "    Y = vectorizer.transform([d])\n",
    "    prediction = model.predict(Y)\n",
    "    if str(prediction) not in freq:\n",
    "        freq[str(prediction)] = 1\n",
    "    else:\n",
    "        freq[str(prediction)] += 1\n",
    "\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "An update on #COVID19 situation in Canada with #PHAC officials will be broadcast live today at 3:00PM EDT, on @GovCanHealth and on the Healthy Canadians Facebook channel: https://t.co/655ckkbYDX\n- - - - - - - - - -\nAn update on #COVID19 situation in Canada with #PHAC senior officials will be broadcast live today at 12 PM EDT, on @GovCanHealth and on the Healthy Canadians Facebook channel: https://t.co/655ckkbYDX\n- - - - - - - - - -\nAn update on #COVID19 situation in Canada with #PHAC senior officials will be broadcast live today at 12:00 EDT, on @GovCanHealth and on the Healthy Canadians Facebook channel: https://t.co/655ckkbYDX\n- - - - - - - - - -\nAn update on #COVID19 situation in Canada, including #PHAC senior officials will be broadcast live today at 1:30PM EDT, on @GovCanHealth and on the Healthy Canadians Facebook channel: https://t.co/655ckkbYDX\n- - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "nums = []\n",
    "for num, d in enumerate(documents):\n",
    "    Y = vectorizer.transform([d])\n",
    "    prediction = model.predict(Y)\n",
    "    if len(tweets) < 20 and prediction == [4]:\n",
    "        if str([d])[2:-2] not in tweets:\n",
    "            tweets.append(str([d])[2:-2])\n",
    "            nums.append(num)\n",
    "\n",
    "tweets = []\n",
    "for n in nums:\n",
    "    tweets.append(original_tweets[n])\n",
    "\n",
    "for t in sorted(tweets):\n",
    "    print(t)\n",
    "    print('- - - - - - - - - -')"
   ]
  }
 ]
}